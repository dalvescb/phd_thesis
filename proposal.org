#+TITLE: Thesis Proposal Title
#+DESCRIPTION: Thesis proposal for Curtis D'Alves; McMaster University 2019.
#+AUTHOR: [[mailto:dalvescb@mcmaster.ca][Curtis D'Alves]]
#+EMAIL: curtis.dalves@gmail.com
#+OPTIONS: toc:nil d:nil title:nil
#+PROPERTY: header-args :tangle no :comments link

# At the end of a section, explain why the section is there,
# and what the reader should take away from it.

# MA: LaTeX pads colons, :, with spacing.
# For inline typing annotations, use ghost colon “\:” to avoid this issue.

* Preamble & title page :ignore:

# Top level editorial comments.
#+MACRO: remark  @@latex: \fbox{\textbf{Comment: $1 }}@@

** Minted setup -- colouring code blocks                            :ignore:

#+LATEX_HEADER: \usepackage[]{minted}
#+LATEX_HEADER: \usepackage{tcolorbox}
#+LATEX_HEADER: \usepackage{etoolbox}
#+LATEX_HEADER: \def\mytitle{??? Program Code ???}
#+LATEX_HEADER: \BeforeBeginEnvironment{minted}{\begin{tcolorbox}[title=\hfill \mytitle]}%
#+LATEX_HEADER: \AfterEndEnvironment{minted}{\end{tcolorbox}}%
#+LATEX_HEADER: \usepackage{hyperref}

# Before a code block, write {{{code(title-of-block)}}}
#
#+MACRO: code     #+LaTeX: \def\mytitle{$1}

#+LaTeX: \setminted[haskell]{fontsize=\footnotesize}
#+LaTeX: \setminted[agda]{fontsize=\footnotesize}

# Removing the red box that appears in "minted" when using unicode.
# Src: https://tex.stackexchange.com/questions/343494/minted-red-box-around-greek-characters
#
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \AtBeginEnvironment{minted}{\dontdofcolorbox}
#+LATEX_HEADER: \def\dontdofcolorbox{\renewcommand\fcolorbox[4][]{##4}}
#+LATEX_HEADER: \makeatother
** LaTeX setup                                                      :ignore:

# Hijacking \date to add addtional text to the frontmatter of a ‘report’.
#
#
# DATE: \today\vfill \centerline{---Supervisors---} \newline [[mailto:carette@mcmaster.ca][Jacques Carette]] and [[mailto:kahl@cas.mcmaster.ca][Wolfram Kahl]]

#+LATEX_HEADER: \usepackage[hmargin=25mm,vmargin=25mm]{geometry}
#+LaTeX_HEADER: \setlength{\parskip}{1em}
#+latex_class_options: [12pt]
#+LATEX_CLASS: report-noparts
# Defined below.
#
# Double spacing:
# LaTeX: \setlength{\parskip}{3em}\renewcommand{\baselinestretch}{2.0}
#
#+LATEX_HEADER: \setlength{\parskip}{1em}

#+LATEX_HEADER: \usepackage[backend=biber,style=alphabetic]{biblatex}
#+LATEX_HEADER: \addbibresource{References.bib}

#+LATEX_HEADER: \usepackage{MyUnicodeSymbols}

#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor} % named colours
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \definecolor{darkred}{rgb}{0.3, 0.0, 0.0}
#+LATEX_HEADER: \definecolor{darkgreen}{rgb}{0.0, 0.3, 0.1}
#+LATEX_HEADER: \definecolor{darkblue}{rgb}{0.0, 0.1, 0.3}
#+LATEX_HEADER: \definecolor{darkorange}{rgb}{1.0, 0.55, 0.0}
#+LATEX_HEADER: \definecolor{sienna}{rgb}{0.53, 0.18, 0.09}
#+LATEX_HEADER: \hypersetup{colorlinks,linkcolor=darkblue,citecolor=darkblue,urlcolor=darkgreen}

#+NAME: symbols for itemisation environment
#+BEGIN_EXPORT latex
\def\labelitemi{$\diamond$}
\def\labelitemii{$\circ$}
\def\labelitemiii{$\star$}

% Level 0                 Level 0
% + Level 1               ⋄ Level 1
%   - Level 2       --->      ∘ Level 2
%     * Level 3                   ⋆ Level 3
%
#+END_EXPORT

# Having small-font code blocks.
# LATEX_HEADER: \RequirePackage{fancyvrb}
# LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\scriptsize}

** ~reports-noparts~ LaTeX Class                                    :noexport:

A custom version of the reports class which makes the outermost headings chapters, rather than parts.
#+NAME: make-reports-class
#+BEGIN_SRC emacs-lisp :results none
(add-to-list
  'org-latex-classes
    '("report-noparts"
      "\\documentclass{report}"
      ("\\chapter{%s}" . "\\chapter*{%s}")
      ("\\section{%s}" . "\\section*{%s}")
      ("\\subsection{%s}" . "\\subsection*{%s}")
      ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
      ("\\paragraph{%s}" . "\\paragraph*{%s}")
      ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+END_SRC

Source: Mark Armstrong --github ~armkeh~
** Personal title page                                              :ignore:

#+begin_center org

#+begin_export latex
\thispagestyle{empty}

{\color{white}{.}}

\vspace{5em}

{\Huge Thesis Proposal Title}

\vspace{1em}

{\Large Possibly some more bullshit here}

\vspace{2em}

Department of Computing and Software

McMaster University

\vspace{2em}
\href{mailto:curtis.dalves@gmail.com}{Curtis D'Alves}

\vspace{2em}
\today
#+end_export

\vfill

{{{code({\sc Thesis Proposal \hspace{12em} \color{grey}{.} })}}}
#+begin_src haskell
Christopher Anand                                    anandc@mcmaster.ca
Wolfram Kahl                                         kahl@cas.mcmaster.ca
#+end_src
#+end_center

# LaTeX: \centerline{\sc Draft}

* Abstract and toc                                                   :ignore:
:PROPERTIES:
:CUSTOM_ID: abstract
:END:

# Use:  x vs.{{{null}}} ys
# This informs LaTeX not to put the normal space necessary after a period.
#
#+MACRO: null  @@latex:\null{}@@

#+begin_abstract

In compiler optimization, *Instruction Scheduling* seeks to optimize the order of
a sequence of instructions to maximize throughput while preserving semantics.
As modern pipelined multi-core architectures become increasing more complex, with
techniques such as super-scaling, out of order execution, register remapping,
VLIW (Very Large Instruction Word) and
more scheduling software must itself become more sophisticated to fully
utilize this advanced hardware. However as a known NP-Complete problem, finding
the optimal schedule for a non-trivial program is too costly. Conventional
compilers opt to utilize heuristics that by default yield far from optimal schedules in
exchange for compile time performance. Tuning via compiler flags is complex
requiring expert insight (often the order of flags causes different heuristics
to overwrite one another) and is often ignored by developers. Statically/Dynamically
linked libraries present an opportunity to schedule pre-compiled binaries
outside of compile time making room to use more costly methods to find better
schedules for performance critical blocks.  

\vspace{1em}

This proposal explores techniques for scheduling performance critical code on
modern out-of-order architectures. Previous/Current techniques and their
relevance are detailed, including List Scheduling, Approximation Algorithms,
Stochastic Algorithms and (Non)Linear Programming. Of particular interest, the use of
stochastic algorithms presents opportunity to examine the space of feasible
schedules. We present a constrained non-linear optimization model that relaxes
the discrete nature of scheduling problems. By encoding heuristics as penalty
functions that can be scaled to assign priority and using stochastic-ally
generated scaling parameters we hope to examine the topology of valid schedules
in the more flexible space of $\textrm{R}^n$. 

\vspace{1em}

As a testing bed, we schedule the IBM\textsuperscript{\textregistered}
MASS\texttrademark (Math Acceleration Subsystem) library functions for the
Z\texttrademark and POWER\texttrademark architectures with already successful results,
finding schedules that outperform previous iterations of the library for select
functions.

#+begin_center org
#+begin_small
---Source: https://github.com/dalvescb/phd_thesis ---
#+end_small
#+end_center
#+end_abstract

\newpage
\thispagestyle{empty}
\tableofcontents
\newpage

* Introduction (Background)
  Modern processors are built with pipelined architectures capable of
  Instruction Level Parallelism (ILP), processors capable of exploiting ILP on
  one core through pipelining are known as superscalar processors. Superscalar
  processors present an opportunity to increase execution throughput by a
  multitude of the longest latency instruction of the architecture. However
  maximizing throughput is not always achievable, in part due to instruction
  dependency (i.e the execution of an instruction cannot occur until it's
  operands are available). A sequence of dependent instructions of sufficient
  latency may achieve no benefit from a superscalar architecture. That is, unless they can be
  interleaved with a set of relatively independent instructions. Furthermore,
  resource constraints such as the number of available registers and functional
  units can also delay execution. This creates an interesting problem for
  hardware and/or compilers: selecting a scheduling of instructions.
#+LaTex: \begin{tcolorbox}[title=Problem: Instruction Scheduling]
Given an a dependency graph (DAG) of instructions and resource limitations
(number of registers, functional units, etc), designate an ordering (find a *schedule*) 
maximizing execution throughput 
#+LaTex: \end{tcolorbox}

Even simple formulations of optimal instruction scheduling is an NP-Complete
search problem \parencite{hennessy1983postpass}, and problems attempting to
integrate resource limitations on modern architectures are NP-Hard
\parencite{motwani1995combining}. Thus practical solutions to instruction
scheduling problems are given by either:
    - *Heuristics* most commonly used by conventional schedulers
    - *Approximation Algorithms* some experimental use done for near optimal
      schedules \parencite{costa2016approx}
These solutions have been traditionally defined in hardware, however due to
increasingly complex hardware constraints certain architectures known as Very
Long Instruction Word (VLIW) architectures haven been constructed that move the
burden of scheduling to the compiler \parencite{fisher1983very}. These
algorithms are generally more comprehensive then simple permutation based
heuristics performed at runtime by hardware and take a model of the program
(typically represented as a dependency DAG \parencite{gibbons1986efficient}) as
input and returns an ordered model to be used by the assembler during
compilation.

** Types of Instruction Scheduling
   There are two broad criteria that have historically influenced scheduling
   algorithms: the first being the control flow of the dependency graph, the
   second being the nature/constraints of the architecture being scheduled for
   \parencite{rau1993instruction}. The second criteria yields ad-hoc solutions
   that are difficult to categorize and often forgotten through the evolution of
   architectures. However an effective scheduling algorithm must consider both criteria.

   Within the first criteria (scheduling based on control flow), the following
   categories are worth noting \parencite{rau1993instruction}:
   - *Basic Block:* (local acyclic) break code into blocks within branches (most commonly performed scheduling)
	 - *Global Scheduling:* (global cyclic) schedule across basic block boundaries
	 - *Modulo Scheduling:* (local cyclic) schedules basic blocks inside of a loop, seeking to
     optimize by interleaving iterations
	 - *Trace Scheduling:* (global acyclic/cyclic) tries to optimize control flow by predicting routes
     taken on branches
   Each of the above categories are distinguished by what consideration is given
   to different types of branching. Initial research into scheduling focused
   entirely on local scheduling (ignoring branching)
   \parencite{rau1993instruction} and culminated in the use of various list
   scheduling algorithms in most schedulers by the 80s
   \parencite{fisher1983very}. An intuitive approach to global scheduling is to
   first schedule basic blocks then attempt to move operations from one block
   to an empty slots in neighboring blocks. However this approach would need to
   take into account/possibly reverse too many arbitrary decisions made in local
   scheduling in every possible neighboring block. To compensate this for this,
   techniques for predicting more frequently occurring branch routes to improve
   global scheduling was invented known as trace scheduling \parencite{fisher1981trace}.
   Cyclic scheduling deals with branching that conforms to a loop in the control
   graph, and could be dealt with in the same fashion as global/trace
   scheduling, however is important/distinct enough to have it's own class of
   algorithm known as modulo scheduling (discussed in a later section).
*** TODO COMMENT reference list and modulo scheduling section
    
** SuperScalar Architectures
   
   #+BEGIN_SRC ditaa :file figures/RISCPipeline.png
   /----------+----------+----------+----------+----------\
   |       IF |       ID |       EX |      MEM | cBLU  WB |
   \----------+----------+----------+----------+----------+----------\ 
              |       IF |       ID |       EX | cBLU MEM |       WB |
              \----------+----------+----------+----------+----------+----------\
   |                     |       IF |       ID | cBLU  EX |      MEM |       WB |
   v                     \----------+----------+----------+----------+----------+----------\
   instr. i                         |       IF | cBLU  ID |       EX |      MEM |       WB |
                                    \----------+----------+----------+----------+----------+----------\
   --->                                        | cBLU  IF |       ID |       EX |      MEM |       WB |
     cycle t                                   \----------+----------+----------+----------+----------/
  
   #+END_SRC

   #+ATTR_LATEX: :width 1.0\textwidth 
   #+CAPTION: Simple RISC Pipelined Architecture
   #+LABEL: fig:PipelinedArchitecture
   #+RESULTS:
   [[file:figures/RISCPipeline.png]]
   
  
   Simple pipelined architectures issue a single instruction per "cycle".
   Through pipeling, ILP is still exploitable, but limited by only having one of
   each type of functional unit. Figure [[fig:PipelinedArchitecture]] show's an
   example simple RISC architecture that exploits ILP while using only a single
   execution unit. Conversely, figure [[fig:SuperScalarArchitecture]] shows a
   superscalar architecture that utilizes parallel execution units.  
   
   #+BEGIN_SRC ditaa :file figures/SuperScalarPipeline.png
   /----------+----------+----------+----------+----------\
   |       IF |       ID |       EX |      MEM | cBLU  WB |
   +----------+----------+----------+----------+----------+ 
   |       IF |       ID |       EX |      MEM | cBLU  WB |
   \----------+----------+----------+----------+----------+----------\ 
              |       IF |       ID |       EX | cBLU MEM |       WB |
              +----------+----------+----------+----------+----------+
              |       IF |       ID |       EX | cBLU MEM |       WB |
              \----------+----------+----------+----------+----------+----------\
  |                      |       IF |       ID | cBLU  EX |      MEM |       WB |
  v                      +----------+----------+----------+----------+----------+
  instr i                |       IF |       ID | cBLU  EX |      MEM |       WB |
                         \----------+----------+----------+----------+----------+----------\
                                    |       IF | cBLU  ID |       EX |      MEM |       WB |
  --->                              +----------+----------+----------+----------+----------+
     cycle t                        |       IF | cBLU  ID |       EX |      MEM |       WB |
                                    \----------+----------+----------+----------+----------+----------\
                                               | cBLU  IF |       ID |       EX |      MEM |       WB |
                                               +----------+----------+----------+----------+----------+
                                               | cBLU  IF |       ID |       EX |      MEM |       WB |
                                               \----------+----------+----------+----------+----------/
   #+END_SRC

   #+ATTR_LATEX: :width 1.0\textwidth 
   #+CAPTION: SuperScalar Architecture   
   #+LABEL: fig:SuperScalarArchitecture
   #+RESULTS:
   [[file:figures/SuperScalarPipeline.png]]
   
  
   Superscalar architectures are all uniprocessors that can execute two or more
   scalar operations in parallel, this encompasses a wide variety of
   architectures, but common to all these architectures are the existense of
   parallel and pipelined functional units, and the need to manage that
   parallelism \parencite{zyuban2001inherently}. In particular, superscalar
   architectures put increased strain on resource management. This poses a more
   serious challenge for scheduling algorithms, since basic block scheduling is
   often not sufficient to allow full utilization of machine
   resources \parencite{bernstein1991global}. 

   An ideal architecture to schedule for would be a RISC architecture with
   a collection of functional units of $m$ types, where the machine has $n_1$,
   $n_2$, ..., $n_m$ units of each type. One could view optimizing a schedule over such
   an architecture as maximizing the amount of live functional units per
   cycle (i.e maximum throughput). This would generally be accomplished by
   interleaving different types of instructions, however if you stretch data
   dependent instructions too far apart doing this you risk running out of
   available registers (you increase register pressure). 
   
*** TODO COMMENT update superscalar architectures from beamer
** Pipeline Stalls
   Both of the previous sections figures [[fig:PipelinedArchitecture]] and
   [[fig:SuperScalarArchitecture]] show an ideal schedule with *NO* stalls. A stall
   occurs when, for whatever reason, full throughput cannot be achieved and a
   NOOP (No-Operation) instruction must be inserted. This is also known as
   inserting a bubble in the pipeline. 
#+LaTex: \begin{figure}[!h]
  #+BEGIN_center
  #+ATTR_LATEX: :width 0.4\textwidth :center
    [[file:figures/bubbles.png]]
  #+ATTR_LATEX: :width 0.46\textwidth :center
    [[file:figures/bubbles2.png]]
  #+END_center 
#+LaTex: \label{fig:PipelineStall}
#+LaTex: \caption{Example of a bubble (NOP) being inserted to fix an unfullfilled data dependency}
#+LaTex: \end{figure}
   Figure \ref{fig:PipelineStall} gives an
   example of inserting a bubble.
*** TODO COMMENT update pipeline stalls from beamer
** Hazards
		- *Data Hazards*
			- read after write *RAW*
			- write after read *WAR*
			- write after write *WAW*
		- *Structural Hazards* occurs when an aspect of hardware is accessed at the same time
		- *Control Hazards* caused by branching, next instruction unknown
    Hardware encountering hazards causees stalls in the pipeline
*** TODO COMMENT update hazards from beamer
   
** Register Allocation
   \parencite{Chaitin:1982:RAS:872726.806984} TODO REFERENCE REGISTER ALLOC
   - Given a schedule, assign registers keeping in mind
    - limited number of registers
    - can't rewrite a register until consumed by dependent instructions
   - Known NP-Complete
     - Practically solved using non-optimal *Graph Coloring* algorithms
     - done seperately from instruction scheduling (before or afterwords)
*** TODO COMMENT update register allocation from beamer
** Graph Coloring
   
#+BEGIN_SRC ditaa :file figures/GraphColor.png
 Code Seq.       Interference Graph            Allocation

A <- ...     /--------\         /--------\     R1 <- ...
B <- ...     | cGRE A |         | cGRE D |     R2 <- ...
    B ...    \--------/         \--------/         R2 ...
C <- ...         |   |               |         R2 <- ...
    A ...        |   \-----------\   |             R1 ...
                 |               |   |
D <- ...     /--------\         /--------\     R1 <- ...
    D ...    | cBLU B |         | cBLU C |         R1 ...
    C ...    \--------/         \--------/         R2 ...
#+END_SRC 

#+CAPTION:Register Allocation via Graph Coloring
#+NAME: fig:GraphColor 
#+RESULTS:
[[file:figures/GraphColor.png]]

   Find a *k-Colouring* for the dependency graph, where *$k = \#Registers$*
*** TODO COMMENT update graph coloring from beamer

** Spilling
   - What if a *k-Coloring* can't be found? Must *Spill* memory
	 - Simply insert new *Load / Store* instructions as needed
   - Potentially *creates new stalls* in the pipeline, need to re-perform
     scheduling
   - May use up dispatch slots
   - An *Ideal Schedule* has no spilling
*** TODO COMMENT update spilling from beamer

** Combining Register Allocation and Instruction Scheduling
   - Register Allocation is generally done after instruction scheduling
   - This can *make spilling necessary*
   - Register allocation can be performed before instruction schedule, but will
     *constrain the space of valid schedules*
   - Attempts to *combine register allocation and scheduling* are
     rare and yield an *NP-hard* problem \parencite{motwani1995combining} \parencite{Pinter:1993:RAI:173262.155114}
*** TODO COMMENT update combining register alloc from beamer
** Swing Modulo Scheduling: Staging

#+BEGIN_SRC ditaa :file figures/SwingModuloStaging.png
                                     
                                     /---------\
                                     |         |
                                     |         v
                                     |   /-----------\
                                     |   |    cBLU   |
                                     |   | Stage 3_1 |
               /---------\           |   \-----------/
               |         |           |         |
               |         v           |         v
               |   /-----------\     |   /-----------\
               |   |    c1AB   |     |   |   c1AB    |
               |   | Stage 2_1 |     |   | Stage 2_2 |
               |   \-----------/     |   \-----------/
               |         |           |         |
               |         v           |         v
/-----------\  |   /-----------\     |   /-----------\
|   c1FF    |  |   |    c1FF   |     |   |   c1FF    | 
| Stage 1_1 |  |   | Stage 1_2 |     |   | Stage 1_3 |
\-----------/  |   \-----------/     |   \-----------/
     |         |         |           |         
     \---------/         \-----------/
#+END_SRC

#+CAPTION:Example 3-Staged for Swing Modulo Scheduling
#+NAME: fig:SwingStaging 
#+ATTR_LATEX: :width 0.6\textwidth
#+RESULTS:
[[file:figures/SwingModuloStaging.png]]

   When performing *modulo scheduling*, a basic block of a loop can be broken
   into stages and the loop can be *unrolled* to interleave stages between
   iterations. The task of generating an optimal, resource-constrained schedule
   for loops with arbitrary recurrences is known to be NP-complete
   \parencite{lam2012systolic}

*** TODO COMMENT update staging from beamer
** Iteration Interval
\begin{equation}
  \frac{\text{latency height}}{\# \text{stages}} \leq \textrm{II}
\end{equation}
   - the maximum *number of cyles* to complete a loop iteration
   - exact number is complicated *Out of Order Execution* / *Staging*
*** TODO COMMENT update II from beamer
** Register Remapping/Renaming
   When executing machine code, hardware maps *Logical Registers* to *Physical Registers*
   -  *Logical Registers* are a set of registers usable directly when
     writing/generating assembly code (limited by system architecture)
   - *Physical Registers* are a set of registers actually available in hardware
   Having a larger number of Physical registers than Logical registers gives
   hardware extra flexibility when dispatching instructions for *Out of Order Execution*
*** TODO COMMENT update register remapping from beamer
** Out-of-Order Execution
   #+BEGIN_SRC ditaa :file figures/OoODiagram.png
   /--------------\      /-------------\
   | Instr 0.     | ...  | Instr. n    |
   \--------------/      \-------------/
         |           |         |
   /--------------\      /-------------\
   | Fetcher 0.   | ...  | Fetcher n   |
   \--------------/      \-------------/
         |           |         |
         |           |         |
         \---------------------/
                     |
                     v
            /-----------------\
            | cBLU Grouper    |           Register Remapping
            \-----------------/
                     |
                     |
                     v 
            /-----------------\
            | cBLU Dispather  |
            \-----------------/
                     |
     -------------------------------------
     |      |                     |      |
   /----\ /----\               /----\ /----\
   |cRED| |cRED|     ....      |cRED| |cRED|    OoO Exection
   \----/ \----/               \----/ \----/
     |      |                     |      |
     -------------------------------------
                     |
                     v 
            /-----------------\
            | cBLU Retire     |           Register UnMapping
            \-----------------/
   #+END_SRC

   #+ATTR_LATEX: :width 0.5\textwidth
   #+RESULTS:
   [[file:figures/OoODiagram.png]]

  Performed genarally in hardware through some variation of *Tomasulo's Algorithm* \parencite{tomasulo1967efficient}

*** TODO COMMENT explain reservation stations
*** TODO COMMENT update out-of-order execution from beamer
    Data flow execution can be controlled in hardware via algorithms such as Tomasulo's algorithm or variations.
     [[https://en.wikipedia.org/wiki/Tomasulo_algorithm#cite_note-intel-5]]
    Necessary Implementation concepts for tomasulo's algorithm
    - *Common data bus* connects reservation stations directly to functional units
    - *Instruction order* instructions are issued sequentially / exceptions are raised sequentially
    - *Register Renaming* 
** Register Pressure In Staged Loops
   - Staging can *increase pipeline throughput* by enabling more instructions to
     be scheduled between high latency operations and subsequent use
   - However this also increases the number of *live instances of loop
     variables* and thus requires more registers to accommodate the schedule
   - To deal with the access number of registers required that may not be
     available, *Register Queuing* (what we term FIFO's) may be
     necessary
   - Existing works have explored schemes of register queuing such as
     *Modulo Variable Expansion* and *Rotating Register
     File* \parencite{tyson2001evaluating}
*** TODO COMMENT update register pressure from beamer
* Current/Previous Approaches
*** TODO COMMENT write intro to current/previous approaches
** List Scheduling (most commonly performed scheduling)
   TODO reference \parencite{hwu1993superblock}
   	Simple heuristic.  Choose a prioritized topological order that
    - Respects the edges in the data-dependence graph (*topological*)
    - Heuristic choice among options, e.g pick first the node with the longest path extending from that node *prioritized*
    Most commonly used method for scheduling. Efficient but yields far less than
    optimal schedules.

    Issues with list scheduling include 
    - Many factors to consider when constructing a schedule (everything listed in this presentation and more!)    
    - Difficult (or more accurately impossible!) to consider all these aspects into a single choice heuristic        
    - Combinations of heuristics can be used, and multiple iterations performed,
      but each will usually undo the work of the other
*** TODO COMMENT update list scheduling from beamer
** Linear/Constraint Programming
     \parencite{malik2008optimal} Found provably optimal schedules for basic blocks using constraint
     programming, with the following types of constraints
   - *Latency Constraints*, i.e
     - Given a labeled dependency DAG $G = (N,E)$ 
       - $\forall (i,j) \in E \cdot j \geq i + l(i,j)$ 
   - *Resource Constraints* that ensured functinonal units were not exceded
   - *Distance Contstraints*, i.e
     - Given a labeled dependency *DAG*  $G = (N,E)$ 
        - $\forall (i,j) \in E \cdot j \geq i + d(i,j)$

   The hard constraints on latency would not account for *Register Remapping* in
   *Out Of Order Execution* that would be able to find more optimal schedules
   despite the fact that latencies in normal execution would create *pipeline stalls*
   {{{code({\sc Assembly Code Example \hspace{12em} \color{grey}{.} })}}}
   #+BEGIN_SRC haskell
   fma r3,r3,r4
   fma r2,r2,r4
   fma r1,r1,r4
   fma r0,r0,r4
   #+END_SRC
   On a system with only 5 registers and an instruction fma of large enough
   latency, the scheduler would push these instructions apart. However a machine
   could use register remapping to execute these instructions efficiently Out-of-Order
   making that constraint unnecessary.
*** TODO COMMENT fix linear/constraint programming from beamer
** Integer Programming
   reference Optimal integer programming \parencite{wilken2000optimal}
*** TODO COMMENT write info on Integer Programming
** Stochastic Search
   Work by stanford \parencite{Schkufza:2016:SPO:2886013.2863701}
  - Suitable for *Short Basic Block* assembly code sequences
  - Utilizes a multiple pass *Stochastic Algorithm*
  - Encodes constraints as a *Cost Function* and uses a
    *Markov Chain Monte Carlo Sampler* to explore space of all
    possible schedules

  Each pass of the optimization minimizes the cost function

  \begin{equation*}
    cost(R; T) = w_e \times eq(R; T) + w_p \times perf(R; T)
  \end{equation*}

  | $\color{lightgreen}{\boldsymbol{R}}$   | any rewrite of the program                                        |
  | $\color{lightgreen}{\boldsymbol{T}}$   | the input program sequence                                        |
  | $\color{lightgreen}{eq(\cdot)}$        | the equivalence function (0 if $\color{lightgreen}{R \equiv T}$ ) |
  | $\color{lightgreen}{perf(\cdot)}$      | a metric for performance                                          |
  | $\color{lightgreen}{\boldsymbol{w_e}}$ | weight for the equivalence term                                   |
  | $\color{lightgreen}{\boldsymbol{w_p}}$ | weight for the performance term                                   |

  Limitations with the approach as done by \parencite{Schkufza:2016:SPO:2886013.2863701} include
   - Only optimizes basic blocks (no loops)
   - Extremely innefficent (only practical for very short scheduling)
   - Performed in multiple passes with model checking
   - Cost function doesn't model the space of valid checking (hence model
     checking is required per each rewrite)
*** TODO COMMENT update stochastic search from beamer
** Meta-Optimization   
   Previous research into meta-optimization of compilers has been attempted \parencite{stephenson2003meta}.
   Hyper-heuristics are an off-spring of meta-optimization, that search within the search space of just heuristics vs the entire problem solution space.
   TODO reference \parencite{burke2013hyper}
*** TODO COMMENT read and summerize Hyper-Heuristics paper [[https://orsociety.tandfonline.com/doi/full/10.1057/jors.2013.71?casa_token=fOf2wR5Su64AAAAA%3A69plSPDMUXUurTufXWal6lCO6_73-XTubToX-9HY09raeRuaCwbO2SIre-CKBCBYHjsLFWBM4os#.XfFyqXWYUUG]]
*** TODO COMMENT read and sumemrize ML for iterative optimizaiton slides [[https://www.eecis.udel.edu/~cavazos/cgo-2006-talk.pdf]]
* Proposed Approaches
** TODO COMMENT write intro to proposed approaches
** Optimization Model for Modulo Scheduling
\begin{align*}
    \color{lightblue}{\text{Objective Variables }} & t_i, b_i, f_i:& \mathbb{R} \\
    \color{lightblue}{\text{Constants }} & \textrm{II} :& \mathbb{R} \\
    \color{lightblue}{\text{Indicator Function }} & \mathbb{IN} :& \mathbb{R} \rightarrow \mathbb{R} \\
    & t_i :& \text{dispatch time} \\
    & b_i :& \text{completion time} \\
    & f_i :& \text{FIFO use } 0 \leq f_i \leq 1 \\
    & \textrm{II} :& \text{iteration interval} \frac{\# instructions}{dispatches/cycle} \\
\end{align*}

\begin{align}
    \color{lightblue}{\text{Hard Constraints }} \qquad & \forall i,j \cdot i \rightarrow j \qquad t_i + \epsilon \leq t_j  \\
								 & 0 \leq t_i \leq b_i \leq \#\text{stages} \cdot \textrm{II}  \\
								 & b_i + \epsilon \leq t_i + \textrm{II} \\
    \color{lightblue}{\text{Objective Function }} \qquad   & \text{min} \sum_{i} (b_i - t_i + f_i) + \text{Penalties}
\end{align}

*Key Idea:* Encode choice heuristics as penalties, adjust preference
between heuristics by scaling
*** TODO COMMENT update optimization model from beamer
** IO Penalty
   - *IDEA* penalize dispatch time of instructions based on the quantity and
    latencies of it's dependencies
   - *Note* This is a *penalty* not a *hard* constraint on latencies
   \begin{align*}
            \color{lightblue}{\text{Given }} \qquad  & t_i,t_j \qquad & \forall i,j \mid i \rightarrow j  \\
            \color{lightblue}{\text{For each i }} \qquad & N_j  =  \sum_{i \rightarrow j} \text{latency}(j) & \\
            \qquad & \qquad & \qquad \\
            \qquad & \mathbb{IO}(i) = \sum_{j} \frac{1}{N_j} \mathbb{IN}(t_i - t_j) & \qquad 
    \end{align*}
*** TODO COMMENT update IO penalty from beamer
** Stochastic Scaling
   - The scaling $\frac{1}{N_j}$ may be a good *guess*, but not necessarily effective in practice
   - *IDEA* scale the *IO penalty* stochastically
   \begin{align*}
    \color{lightblue}{\text{Define a Clustering}} \qquad & \mathbb{C} = \text{Cluster}(\forall i \mid i \rightarrow j) \\
    \color{lightblue}{\text{For each Cluster i}} \qquad & c_i \in \mathbb{RAND(R)} \\
    \color{lightblue}{\text{Stochastic Penalty}} \qquad & \sum_i c_i \cdot \mathbb{IO}(i)
   \end{align*}
*** TODO COMMENT update stochastic scaling from beamer
** Topological Analysis
      *Assertion* For each scaling $\color{lightgreen}{c_i \in \mathbb{RAND(R)}}$, there exists an $\color{lightgreen}{\epsilon \in
     \mathbb(R)}$ such that $\color{lightgreen}{c_i + \epsilon}$
   produces a distinct schedule from $\color{lightgreen}{c_i}$
   - If the assertion fails, the clustering is useless (possible to avoid such
     clusterings?)
   - What does this topology look like?
   - Do all valid schedules span this topology?
   - Prove stochastic scaling spans the topology of all schedules
   - Use PCA analysis to select useful pull parameters
   - Develop clustering methods for assigning pull parameters
 TODO reference topological definition in \parencite{bredon2013topology}
*** TODO COMMENT update topology analysis from beamer
* Timeline
** TODO COMMENT Timeline

* Bib                                                                :ignore:
# LaTeX: \addcontentsline{toc}{section}{References}
#+LaTeX: \addcontentsline{toc}{part}{References}
#+LaTeX: \printbibliography

* Org-Bibtex                                                         :ignore:
** COMMENT PUT BIBTEX ENTRIES HERE IN SUBSECTION ENDED WITH IGNORE USING ORG-BIBTEX-YANK COMMAND :ignore:
** COMMENT EXPORT TO References.bib USING ORG-BIBTEX COMMAND :ignore:
** Meta optimization: improving compiler heuristics with machine learning :ignore:
   :PROPERTIES:
   :TITLE:    Meta optimization: improving compiler heuristics with machine learning
   :BTYPE:    inproceedings
   :CUSTOM_ID: stephenson2003meta
   :AUTHOR:   Stephenson, Mark and Amarasinghe, Saman and Martin, Martin and O'Reilly, Una-May
   :BOOKTITLE: ACM SIGPLAN Notices
   :VOLUME:   38
   :NUMBER:   5
   :PAGES:    77--90
   :YEAR:     2003
   :ORGANIZATION: ACM
   :END:
** Hyper-heuristics: A survey of the state of the art :ignore:
   :PROPERTIES:
   :TITLE:    Hyper-heuristics: A survey of the state of the art
   :BTYPE:    article
   :CUSTOM_ID: burke2013hyper
   :AUTHOR:   Burke, Edmund K and Gendreau, Michel and Hyde, Matthew and Kendall, Graham and Ochoa, Gabriela and {\"O}zcan, Ender and Qu, Rong
   :JOURNAL:  Journal of the Operational Research Society
   :VOLUME:   64
   :NUMBER:   12
   :PAGES:    1695--1724
   :YEAR:     2013
   :PUBLISHER: Taylor \& Francis
   :END:
** An efficient algorithm for exploiting multiple arithmetic units :ignore:
   :PROPERTIES:
   :TITLE:    An efficient algorithm for exploiting multiple arithmetic units
   :BTYPE:    article
   :CUSTOM_ID: tomasulo1967efficient
   :AUTHOR:   Tomasulo, Robert M
   :JOURNAL:  IBM Journal of research and Development
   :VOLUME:   11
   :NUMBER:   1
   :PAGES:    25--33
   :YEAR:     1967
   :PUBLISHER: IBM
   :END:
** The superblock: an effective technique for VLIW and superscalar compilation :ignore:
   :PROPERTIES:
   :TITLE:    The superblock: an effective technique for VLIW and superscalar compilation
   :BTYPE:    incollection
   :CUSTOM_ID: hwu1993superblock
   :AUTHOR:   Hwu, Wen-Mei W and Mahlke, Scott A and Chen, William Y and Chang, Pohua P and Warter, Nancy J and Bringmann, Roger A and Ouellette, Roland G and Hank, Richard E and Kiyohara, Tokuzo and Haab, Grant E and others
   :BOOKTITLE: Instruction-Level Parallelism
   :PAGES:    229--248
   :YEAR:     1993
   :PUBLISHER: Springer
   :END:
** Inherently lower-power high-performance superscalar architectures :ignore:
   :PROPERTIES:
   :TITLE:    Inherently lower-power high-performance superscalar architectures
   :BTYPE:    article
   :CUSTOM_ID: zyuban2001inherently
   :AUTHOR:   Zyuban, Victor V and Kogge, Peter M
   :JOURNAL:  IEEE Transactions on Computers
   :VOLUME:   50
   :NUMBER:   3
   :PAGES:    268--285
   :YEAR:     2001
   :PUBLISHER: IEEE
   :END:
** Very long instruction word architectures and the ELI-512 :ignore:
   :PROPERTIES:
   :TITLE:    Very long instruction word architectures and the ELI-512
   :BTYPE:    book
   :CUSTOM_ID: fisher1983very
   :AUTHOR:   Fisher, Joseph A
   :VOLUME:   11
   :NUMBER:   3
   :YEAR:     1983
   :PUBLISHER: ACM
   :END:
** Trace scheduling: A technique for global microcode compaction  :ignore:
   :PROPERTIES:
   :TITLE:    Trace scheduling: A technique for global microcode compaction
   :BTYPE:    article
   :CUSTOM_ID: fisher1981trace
   :AUTHOR:   Fisher, Joseph A.
   :JOURNAL:  IEEE transactions on computers
   :NUMBER:   7
   :PAGES:    478--490
   :YEAR:     1981
   :PUBLISHER: IEEE
   :END:
** Optimization of horizontal microcode within and beyond basic blocks: an application of processor scheduling with resources :ignore:
   :PROPERTIES:
   :TITLE:    Optimization of horizontal microcode within and beyond basic blocks: an application of processor scheduling with resources
   :BTYPE:    techreport
   :CUSTOM_ID: fisher1979optimization
   :AUTHOR:   Fisher, Joseph A
   :YEAR:     1979
   :INSTITUTION: New York Univ., NY (USA). Courant Mathematics and Computing Lab.
   :END:
** Postpass code optimization of pipeline constraints                :ignore:
   :PROPERTIES:
   :TITLE:    Postpass code optimization of pipeline constraints
   :BTYPE:    article
   :CUSTOM_ID: hennessy1983postpass
   :AUTHOR:   Hennessy, John and Gross, Thomas
   :JOURNAL:  ACM Trans. Program. Lang. Syst.;(United States)
   :VOLUME:   3
   :YEAR:     1983
   :PUBLISHER: Stanford Univ., CA
   :END:
** A systolic array optimizing compiler :ignore:
   :PROPERTIES:
   :TITLE:    A systolic array optimizing compiler
   :BTYPE:    book
   :CUSTOM_ID: lam2012systolic
   :AUTHOR:   Lam, Monica S
   :VOLUME:   64
   :YEAR:     2012
   :PUBLISHER: Springer Science \& Business Media
   :END:
** Topology and geometry :ignore: 
   :PROPERTIES:
   :TITLE:    Topology and geometry
   :BTYPE:    book
   :CUSTOM_ID: bredon2013topology
   :AUTHOR:   Bredon, Glen E
   :VOLUME:   139
   :YEAR:     2013
   :PUBLISHER: Springer Science \& Business Media
   :END:
*** COMMENT [[https://books.google.ca/books?hl=en&lr=&id=wuUlBQAAQBAJ&oi=fnd&pg=PA1&dq=bredon+glen+topology+and+geometry&ots=LFqjujWMGd&sig=fccl_8xgDo7xPGII14WyzTrJaNw#v=onepage&q=bredon%20glen%20topology%20and%20geometry&f=false][Topology and geometry]]
** Constraint-Based Register Allocation and Instruction Scheduling   :ignore:
   :PROPERTIES:
   :TITLE:    Constraint-Based Register Allocation and Instruction Scheduling
   :BTYPE:    phdthesis
   :CUSTOM_ID: castaneda2018constraint
   :AUTHOR:   Casta{\~n}eda Lozano, Roberto
   :YEAR:     2018
   :SCHOOL:   KTH Royal Institute of Technology
   :END:
*** COMMENT [[http://www.diva-portal.org/smash/get/diva2:1232941/FULLTEXT01.pdf][Constraint Based Register allocation and Instruction Scheduling]]   
** Combining register allocation and instruction scheduling          :ignore:
  :PROPERTIES:
  :TITLE:    Combining register allocation and instruction scheduling
  :BTYPE:    article
  :CUSTOM_ID: motwani1995combining
  :AUTHOR:   Motwani, Rajeev and Palem, Krishna V and Sarkar, Vivek and Reyen, Salem
  :JOURNAL:  Courant Institute, New York University
  :YEAR:     1995
  :END:
*** COMMENT [[https://arxiv.org/pdf/1804.02452.pdf][Combining Register Allocation and Instruction Scheduling]]

** Register Allocation with Instruction Scheduling :ignore:
   :PROPERTIES:
   :TITLE:    Register Allocation with Instruction Scheduling
   :BTYPE:    article
   :CUSTOM_ID: Pinter:1993:RAI:173262.155114
   :AUTHOR:   Pinter, Shlomit S.
   :JOURNAL:  SIGPLAN Not.
   :ISSUE_DATE: June 1993
   :VOLUME:   28
   :NUMBER:   6
   :MONTH:    jun
   :YEAR:     1993
   :ISSN:     0362-1340
   :PAGES:    248--257
   :NUMPAGES: 10
   :URL:      http://doi.acm.org/10.1145/173262.155114
   :DOI:      10.1145/173262.155114
   :ACMID:    155114
   :PUBLISHER: ACM
   :ADDRESS:  New York, NY, USA
   :END:
*** COMMENT [[http://delivery.acm.org/10.1145/160000/155114/p248-pinter.pdf?ip=130.113.109.215&id=155114&acc=ACTIVE%20SERVICE&key=FD0067F557510FFB%2ED816932E3DB0B89D%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1564584969_261ecbe26f943fdf33018f2f39ebfbd2][Register Allocation with Instruction Scheduling: A New Approach]]

** Evaluating the use of register queues in software pipelined loops :ignore:
   :PROPERTIES:
   :TITLE:    Evaluating the use of register queues in software pipelined loops
   :BTYPE:    article
   :CUSTOM_ID: tyson2001evaluating
   :AUTHOR:   Tyson, Gary S and Smelyanskiy, Mikhail and Davidson, Edward S
   :JOURNAL:  IEEE Transactions on Computers
   :VOLUME:   50
   :NUMBER:   8
   :PAGES:    769--783
   :YEAR:     2001
   :PUBLISHER: IEEE
   :END:
*** COMMENT [[https://ieeexplore.ieee.org/document/947006][Evaluating the Use of Register Queues in Software Pipelined Loops]]

** Software-pipelining on multi-core architectures :ignore:
   :PROPERTIES:
   :TITLE:    Software-pipelining on multi-core architectures
   :BTYPE:    inproceedings
   :CUSTOM_ID: douillet2007software
   :AUTHOR:   Douillet, Alban and Gao, Guang R
   :BOOKTITLE: Proceedings of the 16th International Conference on Parallel Architecture and Compilation Techniques
   :PAGES:    39--48
   :YEAR:     2007
   :ORGANIZATION: IEEE Computer Society
   :END:
*** COMMENT [[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4336198][Software Pipelining on Multi-core Architectures]]

** Global instruction scheduling for superscalar machines :ignore:
   :PROPERTIES:
   :TITLE:    Global instruction scheduling for superscalar machines
   :BTYPE:    inproceedings
   :CUSTOM_ID: bernstein1991global
   :AUTHOR:   Bernstein, David and Rodeh, Michael
   :BOOKTITLE: ACM SIGPLAN Notices
   :VOLUME:   26
   :NUMBER:   6
   :PAGES:    241--255
   :YEAR:     1991
   :ORGANIZATION: ACM
   :END:
*** COMMENT [[http://pages.cs.wisc.edu/~fischer/cs701.f06/berstein_rodeh.pdf][Global instruction scheduling for superscalar machines]]

** Efficient instruction scheduling for a pipelined architecture :ignore:
   :PROPERTIES:
   :TITLE:    Efficient instruction scheduling for a pipelined architecture
   :BTYPE:    inproceedings
   :CUSTOM_ID: gibbons1986efficient
   :AUTHOR:   Gibbons, Philip B and Muchnick, Steven S
   :BOOKTITLE: Acm sigplan notices
   :VOLUME:   21
   :NUMBER:   7
   :PAGES:    11--16
   :YEAR:     1986
   :ORGANIZATION: ACM
   :END:
*** COMMENT [[http://delivery.acm.org.libaccess.lib.mcmaster.ca/10.1145/20000/13312/p11-gibbons.pdf?ip=130.113.111.210&id=13312&acc=ACTIVE%20SERVICE&key=FD0067F557510FFB%2ED816932E3DB0B89D%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1566799515_cd89aab9c480dc291845f8e0ab01483f][Efficient scheduling for pipelined architectures]]
** Instruction-level parallel processing: history, overview, and perspective :ignore:
   :PROPERTIES:
   :TITLE:    Instruction-level parallel processing: history, overview, and perspective
   :BTYPE:    incollection
   :CUSTOM_ID: rau1993instruction
   :AUTHOR:   Rau, B Ramakrishna and Fisher, Joseph A
   :BOOKTITLE: Instruction-Level Parallelism
   :PAGES:    9--50
   :YEAR:     1993
   :PUBLISHER: Springer
   :END:
*** COMMENT [[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.799.7976&rep=rep1&type=pdf][Instruction-level parallel processing]]
** Register Allocation \& Spilling via Graph Coloring :ignore:
   :PROPERTIES:
   :TITLE:    Register Allocation \& Spilling via Graph Coloring
   :BTYPE:    article
   :CUSTOM_ID: Chaitin:1982:RAS:872726.806984
   :AUTHOR:   Chaitin, G. J.
   :JOURNAL:  SIGPLAN Not.
   :ISSUE_DATE: June 1982
   :VOLUME:   17
   :NUMBER:   6
   :MONTH:    jun
   :YEAR:     1982
   :ISSN:     0362-1340
   :PAGES:    98--101
   :NUMPAGES: 4
   :URL:      http://doi.acm.org.libaccess.lib.mcmaster.ca/10.1145/872726.806984
   :DOI:      10.1145/872726.806984
   :ACMID:    806984
   :PUBLISHER: ACM
   :ADDRESS:  New York, NY, USA
   :END:
*** COMMENT [[http://delivery.acm.org.libaccess.lib.mcmaster.ca/10.1145/810000/806984/p98-chaitin.pdf?ip=130.113.111.210&id=806984&acc=ACTIVE%20SERVICE&key=FD0067F557510FFB%2ED816932E3DB0B89D%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1566800641_adc76422d7bd921a1521c82893f6dceb][Register Allocation]]

** Optimal basic block instruction scheduling for multiple-issue processors using constraint programming :ignore:
  :PROPERTIES:
  :TITLE:    Optimal basic block instruction scheduling for multiple-issue processors using constraint programming
  :BTYPE:    article
  :CUSTOM_ID: malik2008optimal
  :AUTHOR:   Malik, Abid M and McInnes, Jim and Van Beek, Peter
  :JOURNAL:  International Journal on Artificial Intelligence Tools
  :VOLUME:   17
  :NUMBER:   01
  :PAGES:    37--54
  :YEAR:     2008
  :PUBLISHER: World Scientific
  :END:
*** COMMENT [[https://cs.uwaterloo.ca/research/tr/2005/CS-2005-19.pdf][Optimal Basic Block Instruction Scheduling for Multiple Issue Processors Using Constraint Programming]] (IBM guys)
** Optimal instruction scheduling using integer programming :ignore:
   :PROPERTIES:
   :TITLE:    Optimal instruction scheduling using integer programming
   :BTYPE:    inproceedings
   :CUSTOM_ID: wilken2000optimal
   :AUTHOR:   Wilken, Kent and Liu, Jack and Heffernan, Mark
   :BOOKTITLE: Acm sigplan notices
   :VOLUME:   35
   :NUMBER:   5
   :PAGES:    121--133
   :YEAR:     2000
   :ORGANIZATION: ACM
   :END:
*** COMMENT [[http://web.cs.ucla.edu/~palsberg/course/cs239/S04/papers/WilkenLiuHeffernan00.pdf][Optimal scheduling using Integer Programming]]
** MultiLoop: Efficient Software Pipelining for Modern Hardware      :ignore:
   :PROPERTIES:
   :TITLE:    MultiLoop: Efficient Software Pipelining for Modern Hardware
   :BTYPE:    inproceedings
   :CUSTOM_ID: Anand:2007:MES:1321211.1321242
   :AUTHOR:   Anand, Christopher Kumar and Kahl, Wolfram
   :BOOKTITLE: Proceedings of the 2007 Conference of the Center for Advanced Studies on Collaborative Research
   :SERIES:   CASCON '07
   :YEAR:     2007
   :LOCATION: Richmond Hill, Ontario, Canada
   :PAGES:    260--263
   :NUMPAGES: 4
   :URL:      http://dx.doi.org/10.1145/1321211.1321242
   :DOI:      10.1145/1321211.1321242
   :ACMID:    1321242
   :PUBLISHER: IBM Corp.
   :ADDRESS:  Riverton, NJ, USA
   :END:
*** COMMENT [[https://link.springer.com/content/pdf/10.1007%2F978-1-4899-7797-7_6.pdf][Multi-Loop: Efficient Software Piplining for Modern Hardware]] (Anand,Kahl)

** Stochastic Program Optimization :ignore:
   :PROPERTIES:
   :TITLE:    Stochastic Program Optimization
   :BTYPE:    article
   :CUSTOM_ID: Schkufza:2016:SPO:2886013.2863701
   :AUTHOR:   Schkufza, Eric and Sharma, Rahul and Aiken, Alex
   :JOURNAL:  Commun. ACM
   :ISSUE_DATE: February 2016
   :VOLUME:   59
   :NUMBER:   2
   :MONTH:    jan
   :YEAR:     2016
   :ISSN:     0001-0782
   :PAGES:    114--122
   :NUMPAGES: 9
   :URL:      http://doi.acm.org/10.1145/2863701
   :DOI:      10.1145/2863701
   :ACMID:    2863701
   :PUBLISHER: ACM
   :ADDRESS:  New York, NY, USA
   :END:
*** COMMENT [[http://delivery.acm.org/10.1145/2870000/2863701/p114-schkufza.pdf?ip=130.113.109.215&id=2863701&acc=ACTIVE%20SERVICE&key=FD0067F557510FFB%2ED816932E3DB0B89D%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1564586602_105c24f842dcdd9a6b420b8bd3191e66][Stochastic Program Optimization]]

** Kristons Thesis  :ignore:
   :PROPERTIES:
   :TITLE: Approximation Algorithm based Approach Instruction Scheduling
   :BTYPE: article
   :CUSTOM_ID: costa2016approx
   :AUTHOR: Kriston Costa
   :URI: http://hdl.handle.net/11375/18865  
   :PUBLISHER: MacSphere
   :END:
*** COMMENT [[https://macsphere.mcmaster.ca/bitstream/11375/18865/2/costa_kriston_p_201602_msc.pdf][Approximation Algorithm based Approach Instruction Scheduling]] (Kriston's thesis)
* COMMENT footer                                                     :ignore:

# Local Variables:
# eval: (progn (org-babel-goto-named-src-block "make-reports-class") (org-babel-execute-src-block) (outline-hide-sublevels 1))
# compile-command: (progn (org-babel-tangle) (org-latex-export-to-pdf) (async-shell-command "evince proposal.pdf"))
# End:
